{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psuedo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mctx\n",
    "import jax as jx\n",
    "from optimizers import adamw\n",
    "batchsize = 32\n",
    "config = \"some file\"\n",
    "class V_function():\n",
    "    def __init__(self,config):\n",
    "        self.num_hidden_units = 200\n",
    "        self.num_hidden_layers = 3\n",
    "        self.relu = nn.relu()\n",
    "    def forward(self, obs):\n",
    "      #obs::(board_h,board_w,action_num)\n",
    "      x = np.flatten(obs)   #h*w*A\n",
    "      for i in range(self.num_hidden_layers):\n",
    "         x = self.relu(F.linear(self.num_hidden_units)(x))\n",
    "      V = F.linear(1)(x)   #ex. [0.23]\n",
    "      return V[0]   #ex. 0.23\n",
    "    \n",
    "class pi_function():\n",
    "    def __init__(self,config,num_actions):\n",
    "        self.num_hidden_units = 200\n",
    "        self.num_hidden_layers = 3\n",
    "        self.relu = nn.relu()\n",
    "        self.num_actions = num_actions\n",
    "    def forward(self, obs):\n",
    "      #obs::(board_h,board_w,action_num)\n",
    "      x = np.flatten(obs)   #h*w*A\n",
    "      for i in range(self.num_hidden_layers):\n",
    "         x = self.relu(F.linear(self.num_hidden_units)(x))\n",
    "      pi_logit = F.linear(self.num_actions)(x)   #ex. [0.23]\n",
    "      return pi_logit   #ex. 0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recurrent_fn(env, V_func, pi_func):\n",
    "    def recurrent_fn(actions, env_states):\n",
    "        env_states, obs, rewards, terminals, _ = env.step(actions, env_states)   #batch_axes=(0,0)\n",
    "        V = V_func(obs)     #batch_axes=(0)\n",
    "        pi_logit = pi_func(obs)  #batch_axes=(0)\n",
    "        recurrent_fn_output = mctx.RecurrentFnOutput(\n",
    "            reward=rewards,\n",
    "            discount=(1.0-terminals)*0.99,    #0.99\n",
    "            prior_logits=pi_logit,\n",
    "            value=V\n",
    "        )\n",
    "        return recurrent_fn_output, env_states\n",
    "    return recurrent_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fn(env,key):\n",
    "    #* environment\n",
    "    key, subkeys_batch = jx.random.split(key,batchsize);     \n",
    "    env_states = env.reset(subkeys_batch)   #batched version state  env_states: [bs, ....]\n",
    "    num_actions = env.num_actions()  #5\n",
    "    #*  v_net \n",
    "    V_func =  V_function(config)\n",
    "    #* p_net\n",
    "    pi_func = pi_function(config,num_actions)\n",
    "\n",
    "    #* v_optim\n",
    "    optimV = V_opt_state,V_opt_update, get_V_params = adamw(V_func.parameters())\n",
    "    #* p_optim\n",
    "    optimP = pi_opt_state, pi_opt_update, get_pi_params = adamw(pi_func.parameters())\n",
    "                        \n",
    "    return env_states, V_func, pi_func,    optimV, optimP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math.log as log\n",
    "import tc.nn.softmax as softmax\n",
    "  \n",
    "class AC_loss():\n",
    "    def __init__(self,pi_func, V_func,):\n",
    "        self.pi_func = pi_func\n",
    "        self.V_func = V_func\n",
    "    def forward(self, pi_MCTS, V_MCTS, obs):\n",
    "        pi_logits = self.pi_func(obs)\n",
    "        V = self.V_func(obs)\n",
    "\n",
    "        pi_loss = sum(pi_MCTS*(log(pi_MCTS/softmax(pi_logits))))   #entropyloss =  y*log(y/y_hat)\n",
    "        V_loss = (V_MCTS-V)**2                         #  MSE\n",
    "\n",
    "        return sum(pi_loss+V_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 0\n",
    "import jax_environments\n",
    "key = jx.random.PRNGKey(0)\n",
    "\n",
    "Environment = getattr(jax_environments, \"ProcMaze\")   #*ProcMaze   num_action == 5  \n",
    "env_config = config.env_config \n",
    "env = Environment( grid_size=5, timeout=64)\n",
    "num_actions = env.num_actions()   #5\n",
    "key, subkey = jx.random.split(key)\n",
    "\n",
    "env_states, V_func, pi_func,  optim_V, optim_P = init_fn(env,subkey)\n",
    "\n",
    "\n",
    "get_V_params = optim_V.get_V_params;   V_opt_state   = optim_V.V_opt_state\n",
    "V_target_params = V_func.parameters()\n",
    "\n",
    "get_pi_params = optim_P.get_pi_params;  pi_opt_state = optim_P.pi_opt_state\n",
    "\n",
    "\n",
    "recurrent_fn = get_recurrent_fn(env, V_func, pi_func)\n",
    "iterations = 100\n",
    "\n",
    "import jax.grad as grad\n",
    "import functools\n",
    "\n",
    "Ac = AC_loss(pi_func, V_func)\n",
    "AC_loss_model = grad(Ac,arg=(Ac.pi_func.params,Ac.V_func.params))    #*   AC_Loss(.. .,pi_MCTS, V_target, obs    #batch_axis  (...,0,0,0)  \n",
    "\n",
    "def agent_environment_interaction_loop_function(S):\n",
    "    S[\"key\"], subkey = jx.random.split(S[\"key\"])\n",
    "    for _ in range(iterations):\n",
    "        obs = env.get_observation(S[\"env_states\"])   #*  get observation in the real world\n",
    "        #**  turn env_states = (goar,wall_grid,pos,t)  to  np.array\n",
    "        #**  next env_state = env.step(env_states)\n",
    "        pi_logits = pi_func(obs)  #pi(obs)   batch_axes=(0,)\n",
    "        V = V_func(obs)  #V(obs)  batch_axes=(0,)\n",
    "\n",
    "        root = mctx.RootFnOutput(\n",
    "            prior_logits=pi_logits,\n",
    "            value=V,\n",
    "            embedding=S[\"env_states\"]\n",
    "        )\n",
    "\n",
    "        S[\"key\"], subkey = jx.random.split(S[\"key\"])\n",
    "        #** run simutions  32 times  and get a policy_output\n",
    "        policy_output = mctx.gumbel_muzero_policy(\n",
    "            params={\"V\":S[\"V_target_params\"], \"pi\":get_pi_params(S[\"pi_opt_state\"])},\n",
    "            rng_key=subkey,\n",
    "            root=root,\n",
    "            recurrent_fn=recurrent_fn,\n",
    "            num_simulations=config.num_simulations,   #10\n",
    "            max_num_considered_actions=num_actions,   #5\n",
    "            qtransform=functools.partial(\n",
    "                mctx.qtransform_completed_by_mix_value,\n",
    "                use_mixed_value=config.use_mixed_value,  #true\n",
    "                value_scale=config.value_scale       #0.1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # tree search derived targets for policy and value function\n",
    "        search_policy = policy_output.action_weights    #policy: policy_output.action_weights \n",
    "                                                        #action: policy_output.action\n",
    "        search_value = policy_output.search_tree.qvalues(ROOT_INDEX=0)[policy_output.action]\n",
    "        \n",
    "\n",
    "        # compute loss gradient compared to tree search targets and update parameters\n",
    "        #                               AC_Loss()        pi_params V_params, pi_target,V_target, obs\n",
    "        pi_grads, V_grads = AC_loss_model(search_policy, search_value, obs)  #forward(self, pi_MCTS, V_MCTS, obs):\n",
    "        S[\"pi_opt_state\"] = optim_P(pi_grads,S[\"pi_opt_state\"]); \n",
    "        S[\"V_opt_state\"]  = optim_V(V_grads,S[\"V_opt_state\"]); \n",
    "\n",
    "        # Update V target params after a particular number of parameter updates   \n",
    "        S[\"opt_t\"]+=1\n",
    "        if S[\"opt_t\"]%config.target_update_frequency == 0:\n",
    "            S[\"V_target_params\"] = get_V_params(S[\"V_opt_state\"])\n",
    "\n",
    "        # always take action recommended by tree search\n",
    "        actions = policy_output.action\n",
    "\n",
    "        # step the environment  #*  take real action in real environment\n",
    "        S[\"env_states\"], obs, reward, terminal, _ = env.step(actions, S[\"env_states\"])   #batch_axes=(0,0)  env.step(action,env_state)\n",
    "\n",
    "        # reset environment if terminated\n",
    "        S[\"key\"], subkeys = jx.random.split(S[\"key\"])\n",
    "        if terminal: \n",
    "            S[\"env_states\"] = env.reset(subkeys)                  \n",
    "\n",
    "        #*  reward  and return  in the real environment\n",
    "        # update statistics for computing average return\n",
    "        S[\"episode_return\"] += reward\n",
    "        if terminal:\n",
    "            S[\"avg_return\"] = S[\"avg_return\"]*0.9+S[\"episode_return\"]*0.1\n",
    "            S[\"episode_return\"] =  0\n",
    "            S[\"num_episodes\"] =  S[\"num_episodes\"]+1\n",
    "    return S\n",
    "\n",
    "\n",
    "run_state = {\"env_states\":env_states, \n",
    "            \"V_opt_state\":V_opt_state,  \"V_target_params\":V_target_params, \n",
    "            \"pi_opt_state\":pi_opt_state, \n",
    "            \"opt_t\":0,  \"avg_return\":0, \"episode_return\":0, \n",
    "            \"num_episodes\":0, \"key\":key}\n",
    "avg_returns = []\n",
    "times = []\n",
    "\n",
    "for i in range(config.num_steps//config.eval_frequency):\n",
    "    # perform a number of iterations of agent environment interaction including learning updates\n",
    "    run_state = agent_environment_interaction_loop_function(run_state)\n",
    "\n",
    "    # avg_return is debiased, and only includes batch elements wit at least one completed episode so that it is more meaningful in early episodes\n",
    "    valid_avg_returns = run_state[\"avg_return\"][run_state[\"num_episodes\"]>0]\n",
    "    valid_num_episodes = run_state[\"num_episodes\"][run_state[\"num_episodes\"]>0]\n",
    "    avg_return = np.mean(valid_avg_returns/(1-config.avg_return_smoothing**valid_num_episodes))\n",
    "    print(\"Running Average Return: \"+str(avg_return))\n",
    "    avg_returns+=[avg_return]\n",
    "\n",
    "    time_step+=config.eval_frequency\n",
    "    times+=[time_step]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
