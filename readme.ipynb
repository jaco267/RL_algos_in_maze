{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compare RL-algos\n",
    "Compare the performance and training speed between Q-learning and Muzero.   \n",
    "### Conclusion\n",
    "Muzero learns pretty slow, but is more powerful, because you can increase the simulation budget to solve more difficult environment, with the cost of more training time.  (which (I guess) is the main reason why [mctx](https://github.com/google-deepmind/mctx) using jax instead of something like tensorflow or pytorch, because jax's xla will speed up the training time, and treemap is also convenient for building mcts)       \n",
    "### acknowledge     \n",
    "code and algos are from these projects\n",
    "- [mctx](https://github.com/google-deepmind/mctx)  for gumbel muzero\n",
    "- [mctx_learning_demo](https://github.com/kenjyoung/mctx_learning_demo)  for maze environment\n",
    "- [jax-rl](https://github.com/erees1/jax-rl)  for q-learning     \n",
    "     \n",
    "### results\n",
    "#### in 5x5 maze:    \n",
    "network-structure--> [100]*3      \n",
    "gumbel muzero (with 5 simu budget) and Qlearning can both solve the 5x5 maze (because it's a simple problem).       \n",
    "But gumbel muzero (with 5 simu budget) takes more times to solve the problem than Q-learning.    \n",
    "gumbel muzero (with 1 simu budget) learns significant slower than q-learning\n",
    "#### in 7x7 maze: ( more difficult environment.)\n",
    "network-structure--> [100]*5      \n",
    "Q learning fails to learn anything useful            \n",
    "gumbel muzero can learn something     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5x5 maze  (simple problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python algos/gumbel_zero/main.py  --enable_wandb True --env_config.grid_size 5 --num_simulations 5 --algo gumbel_mu5x5_simu5\n",
    "python algos/q_learn/main.py  --enable_wandb True --env_config.grid_size 5 --algo q_learn5x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gumbel_muzero(5 simulations) vs dqn's reward,  each step's value is 10 second    \n",
    "gumbel muzero (with 5 simu budget) converge slower than dqn, but both algos can solve this simple 5x5 maze.                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/gumbel_q_learn.png\" alt= “” width=\"800px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python algos/gumbel_zero/main.py  --enable_wandb True --env_config.grid_size 5 --num_simulations 3 --algo gumbel_mu5x5_simu3\n",
    "python algos/gumbel_zero/main.py  --enable_wandb True --env_config.grid_size 5 --num_simulations 1 --algo gumbel_mu5x5_simu1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gumbel muzero with one simution budget (orange) learns significantly slower than q_learnging   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/simu_3_1.png\" alt= “” width=\"800px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7x7 maze  (more difficult problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python algos/gumbel_zero/main.py  --enable_wandb True --env_config.grid_size 7 --num_hidden_layers 5 --num_simulations 16  --algo gumbel_mu7x7_simu16_net_100x5\n",
    "\n",
    "python algos/gumbel_zero/main.py  --enable_wandb True --env_config.grid_size 7 --num_hidden_layers 5 --num_simulations 8  --algo gumbel_mu7x7_simu8_net_100x5\n",
    "\n",
    "python algos/gumbel_zero/main.py  --enable_wandb True --env_config.grid_size 7 --num_hidden_layers 5 --num_simulations 4  --algo gumbel_mu7x7_simu4_net_100x5\n",
    "\n",
    "\n",
    "python algos/q_learn/main.py  --enable_wandb True --env_config.grid_size 7 --n_layers 5  --algo q_learn7x7_net_100x5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q-learning fails to learn anythin useful     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/7x7.png\" alt= “” width=\"800px\" >"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
